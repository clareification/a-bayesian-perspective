# -*- coding: utf-8 -*-
"""learning_model_weights.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGjRBgPlTS_9gnb-iJ9-qqXXe1-4cLV7
"""

import torch 
import torch.nn as nn
import numpy as np 
import torchvision
import matplotlib.pyplot as plt
from model_zoo import *
from data_loaders import get_FMNIST, get_synthetic_data



def train_step(x, y, m, o, criterion):
    o.zero_grad()
    out = m(x)
    l = criterion(out, y.reshape(-1))
    l.backward()
    o.step()
    out2 = m(x)
    l2 = criterion(out2, y.reshape(-1))
    if np.random.rand() > 0.9:
        print(l - l2)
    # print(torch.norm(out - out2))
    return out, l

def linear_train_step(outs, w, y, criterion, model_backprop=False, model_opts=None, models=None, x=None, step_size=0.01):
    outs = torch.stack(outs, axis=1).detach().numpy()
    outs = outs.T

    outs = torch.tensor(outs)
    w_out = torch.einsum('abc, b->ac', outs, w)
    
    w_out = w_out.T
    l = criterion(w_out, y)

    l.backward()
    g = w.grad.data

    with torch.no_grad():
        w = w - (step_size*g)
    
    if model_backprop:
        for i, o in enumerate(model_opts):
            # local_out = models[i](x)
            # local_ou
            o.step()
    return l
    

def train_parallel_one_epoch(w, ms, opts, training_data, step_size=0.001, loss_fn=torch.nn.CrossEntropyLoss):
    # Train weights and models concurrently.
    # Don't backprop linear loss to models
    M = len(ms)
    losses = [[] for m in ms]
    criterion = loss_fn()

    lin_losses = []
    for data in training_data:     
        y = data[1]

        x = data[0].reshape([-1, 784])
        outs = []
        # Take one optimization step for each model
        for i, (m, o) in enumerate(zip(ms, opts)):
            out, l = train_step(x, y, m, o, criterion)
            outs.append(out)
            losses[i].append(l.detach().item())

        # Linear step    
        
        # linear_train_step(outs, w, y, criterion)
        # w.requires_grad = True
        
        # lin_losses.append(l.detach().item())
    return lin_losses, losses, w


def train_combo_one_epoch(w, ms, opts, training_data, step_size=0.001, loss_fn=torch.nn.CrossEntropyLoss):
    # Train weights and models concurrently.
    # DO backprop linear loss to models
    M = len(ms)
    losses = [[] for m in ms]
    criterion = loss_fn()

    lin_losses = []
    for data in training_data:     
        y = data[1]

        x = data[0].reshape([-1, 784])
        outs = []

        # Compute model outputs
        for i, (m, o) in enumerate(zip(ms, opts)):
            o.zero_grad()
            out = m(x)
            outs.append(out)

        # Compute loss of linear layer and backprop         
        l = linear_train_step(outs, w, y, criterion, True, model_opts=opts)

        w.requires_grad = True
        
        lin_losses.append(l.detach().item())
    return lin_losses, w

if __name__ == "__main__":
    # Load dataset

    # Train one epoch 
    
    _, _, listl, tl = get_FMNIST()
    listl = list(listl)[:10]


    dim_in = 784
    dim_out=10
    models = []
    optims = []
    widths = [1, 10, 20, 100, 500, 1000]
    for h in widths:    
        m = TwoLayer(dim_in, h, dim_out)
        models.append(m)
        o = torch.optim.SGD(m.parameters(), lr=0.1)
        optims.append(o)


    lin = 1/len(models) * torch.ones(len(models))
    print(lin.requires_grad)
    lin.requires_grad = True
    print(lin.requires_grad)
    lin_opt = torch.optim.SGD([lin], lr=0.01)
    l2s = [[] for _ in models]
    for _ in range(150):
        l2, lin = train_combo_one_epoch(lin, models, optims, listl)
        l2s = [s + l2 for i, s in enumerate(l2s)]
    l2 = l2s

    ps = lin
    print(ps)
    for i,ls in enumerate(l2):
        print(ls[-1])
        plt.plot(ls[1:], label='w=' + str(widths[i]))
    plt.show()
    #print(l[-1]) 
    i = [sum(z) for z in l2]
    print(i)
    print(lin.detach().numpy().reshape(-1))

    # plt.plot(l[10:], label='lin')
    # plt.legend()
    # plt.show()

    sums = [sum(z) for z in l2]
    plt.scatter(sums, lin.detach().numpy().reshape(-1))
    plt.show()

    mean_l = []
    # for j, m in enumerate(models):
    #     criterion = torch.nn.CrossEntropyLoss()
    # l = 0
    # # Compute test accuracy
    # for i, dat in enumerate(tl):
    #     out = m(dat[0].reshape(-1, 784))
    #     l += criterion(out, dat[1])
    # mean_l.append(l/i)
