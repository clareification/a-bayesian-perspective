# -*- coding: utf-8 -*-
"""learning_model_weights.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGjRBgPlTS_9gnb-iJ9-qqXXe1-4cLV7
"""

import torch 
import torch.nn as nn
import numpy as np 
import torchvision
import matplotlib.pyplot as plt
from model_zoo import *
from data_loaders import get_FMNIST, get_synthetic_data

_, _, dl, tl = fmnist_data()

dl.dataset[0][0].shape

def train_step(x, y, m, o, criterion):
    o.zero_grad()
    out = m(x)
    l = criterion(out, y.reshape(-1))
    l.backward()
    o.step()
    return out, l

def linear_train_step(outs, w, y, criterion, model_backprop=False, model_opts=None):
    outs = torch.stack(outs, axis=1).detach().numpy()
    outs = outs.T

    outs = torch.tensor(outs)
    w_out = torch.einsum('abc, b->ac', outs, w)
    
    w_out = w_out.T
    l = criterion(w_out, y)

    l.backward()
    g = w.grad.data

    with torch.no_grad():
        w = w - (step_size*g)
    
    if model_backprop:
        for o in model_opts:
            o.step()
    return l
    

def train_parallel_one_epoch(w, ms, opts, training_data, step_size=0.001):
    # Train weights and models concurrently.
    # Don't backprop linear loss to models
    M = len(ms)
    losses = [[] for m in ms]
    criterion = torch.nn.CrossEntropyLoss()

    lin_losses = []
    for data in training_data:     
        if outer_i >= k:
            return lin_losses, losses, w
        y = data[1]

        x = data[0].reshape([-1, 784])
        outs = []
        # Take one optimization step for each model
        for i, (m, o) in enumerate(zip(ms, opts)):
            out, l = train_step(x, y, m, o, criterion)
            outs.append(out)
            losses[i].append(l.detach().item())

        # Linear step    
        
        linear_train_step(outs, w, y, criterion)
        w.requires_grad = True
        
        lin_losses.append(l.detach().item())
    return lin_losses, losses, w


def train_combo_one_epoch(w, ms, opts, training_data, step_size=0.001):
    # Train weights and models concurrently.
    # DO backprop linear loss to models
    M = len(ms)
    losses = [[] for m in ms]
    criterion = torch.nn.CrossEntropyLoss()

    lin_losses = []
    for data in training_data:     
        if outer_i >= k:
            return lin_losses, losses, w
        y = data[1]

        x = data[0].reshape([-1, 784])
        outs = []

        # Compute model outputs
        for i, (m, o) in enumerate(zip(ms, opts)):
            o.zero_grad()
            out = m(o)
            outs.append(out)

        # Compute loss of linear layer and backprop         
        linear_train_step(outs, w, y, opts, criterion)

        w.requires_grad = True
        
        lin_losses.append(l.detach().item())
    return lin_losses, w

if __name__ == "__main__":
    # Load dataset

    # Train one epoch 
    listl = list(dl)

    dim_in = 784
    dim_out=10
    models = []
    optims = []
    widths = [1, 10, 20, 100, 500, 1000]
    for h in widths:    
        m = TwoLayer(dim_in, h, dim_out)
        models.append(m)
        o = torch.optim.SGD(m.parameters(), lr=0.1)
        optims.append(o)


    lin = 1/len(models) * torch.ones(len(models))
    print(lin.requires_grad)
    lin.requires_grad = True
    print(lin.requires_grad)
    lin_opt = torch.optim.SGD([lin], lr=0.01)

    l, l2, lin = train_fmnist_k_steps(lin, lin_opt, models, optims, listl[:10], k=100)

    ps = lin
    print(ps)
    for i,ls in enumerate(l2):
    print(ls[-1])
    plt.plot(ls[19:], label='w=' + str(widths[i]))

    #print(l[-1]) 
    i = [sum(z) for z in l2]
    print(i)
    print(lin.detach().numpy().reshape(-1))

    plt.plot(l[10:], label='lin')
    plt.legend()
    plt.show()

    sums = [sum(z) for z in l2]
    plt.scatter(sums, lin.detach().numpy().reshape(-1))

    mean_l = []
    for j, m in enumerate(models):
    criterion = torch.nn.CrossEntropyLoss()
    l = 0
    # Compute test accuracy
    for i, dat in enumerate(tl):
        out = m(dat[0].reshape(-1, 784))
        l += criterion(out, dat[1])
    mean_l.append(l/i)
